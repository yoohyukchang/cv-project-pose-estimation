{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8863b254-bbc7-4951-bc1e-1b2de170352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad2687b-150b-4c09-a53b-ca2a4bbbe832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current path\n",
    "current_path = Path.cwd()\n",
    "\n",
    "VAL_ANNOTATION_PATH = current_path / \"data\" / \"person_keypoints_val2017.json\"\n",
    "VAL_IMAGE_PATH = current_path / \"data\" / \"val2017\"\n",
    "\n",
    "# Number of keypoints in COCO dataset\n",
    "NUM_KEYPOINTS = 17\n",
    "\n",
    "# Input and output image sizes\n",
    "INPUT_SIZE = (192, 256)   # Width, Height\n",
    "OUTPUT_SIZE = (64, 48)    # Height, Width (for heatmaps)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0726edf-9ab4-41e7-a15e-d83011b187f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de820c57-d98b-4884-94cf-1c60f71429e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonKeypointsDataset(Dataset):\n",
    "    def __init__(self, annotation_file, image_path, min_keypoints=9, max_samples=None, transform=None, target_transform=None, seed=SEED):\n",
    "        \"\"\"\n",
    "        Custom Dataset for COCO Person Keypoints\n",
    "        \"\"\"\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_path = str(image_path)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.person_ids = self.coco.getCatIds(catNms=['person'])\n",
    "        self.img_ids = self.coco.getImgIds(catIds=self.person_ids)\n",
    "\n",
    "        # Set seeds for reproducibility\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # Build list of (image_id, annotation) tuples\n",
    "        self.samples = []\n",
    "        for img_id in self.img_ids:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.person_ids, iscrowd=False)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            for ann in anns:\n",
    "                # Include only samples with enough keypoints\n",
    "                if 'keypoints' in ann and ann['num_keypoints'] >= min_keypoints:\n",
    "                    self.samples.append((img_id, ann))\n",
    "\n",
    "        # Randomly sample if we have more samples than max_samples\n",
    "        if max_samples and max_samples < len(self.samples):\n",
    "            self.samples = random.sample(self.samples, max_samples)\n",
    "            print(f\"Randomly sampled {len(self.samples)} images from {len(self.samples)} total images\")\n",
    "                        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches the image and keypoints for the given index.\n",
    "        \"\"\"\n",
    "        img_id, ann = self.samples[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.image_path, img_info['file_name'])\n",
    "        if not os.path.exists(img_path):\n",
    "            raise RuntimeError(f\"Image file not found: {img_path}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get bounding box\n",
    "        bbox = ann['bbox']  # [x, y, w, h]\n",
    "        x, y, w, h = bbox\n",
    "        x1, y1 = int(x), int(y)\n",
    "        x2, y2 = int(x + w), int(y + h)\n",
    "        \n",
    "        # Crop image to bounding box\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "        \n",
    "        orig_w, orig_h = img.size\n",
    "        \n",
    "        # Adjust keypoints\n",
    "        keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "        keypoints[:, 0] -= x1\n",
    "        keypoints[:, 1] -= y1\n",
    "        \n",
    "        # Resize image and keypoints\n",
    "        new_size = INPUT_SIZE  # (width, height)\n",
    "        img = img.resize(new_size, resample=Image.BILINEAR)\n",
    "        \n",
    "        scale_x = new_size[0] / orig_w\n",
    "        scale_y = new_size[1] / orig_h\n",
    "        keypoints[:, 0] = (keypoints[:, 0] * scale_x)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] * scale_y)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = F.to_tensor(img)\n",
    "\n",
    "        # Convert keypoints to tensor\n",
    "        keypoints = torch.tensor(keypoints, dtype=torch.float32)\n",
    "        \n",
    "        # Generate heatmaps\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(keypoints)\n",
    "        else:\n",
    "            target = torch.tensor(keypoints, dtype=torch.float32)\n",
    "        \n",
    "        meta = {\n",
    "            'image_id': img_id,\n",
    "            'bbox': ann['bbox'],\n",
    "            'scale': (scale_x, scale_y),\n",
    "            'crop_coords': (x1, y1, x2, y2),\n",
    "            'original_image_size': (img_info['width'], img_info['height'])\n",
    "        }\n",
    "        \n",
    "        return img, target, keypoints, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d41bd76-5bb0-4024-aaed-a6fa87c8da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapGenerator:\n",
    "    \"\"\"\n",
    "    Generates heatmaps for each keypoint.\n",
    "    Uses Gaussian distributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, num_keypoints, sigma=2):\n",
    "        self.output_size = output_size  # (height, width)\n",
    "        self.num_keypoints = num_keypoints\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def __call__(self, keypoints):\n",
    "        heatmaps = np.zeros((self.num_keypoints, self.output_size[0], self.output_size[1]), dtype=np.float32)\n",
    "        tmp_size = self.sigma * 3\n",
    "        \n",
    "        for i in range(self.num_keypoints):\n",
    "            kp = keypoints[i]\n",
    "            x, y, v = kp\n",
    "            if v > 0:\n",
    "                x = x * self.output_size[1] / INPUT_SIZE[0]\n",
    "                y = y * self.output_size[0] / INPUT_SIZE[1]\n",
    "                \n",
    "                ul = [int(x - tmp_size), int(y - tmp_size)]\n",
    "                br = [int(x + tmp_size + 1), int(y + tmp_size + 1)]\n",
    "                \n",
    "                if ul[0] >= self.output_size[1] or ul[1] >= self.output_size[0] or br[0] < 0 or br[1] < 0:\n",
    "                    continue\n",
    "                \n",
    "                size = 2 * tmp_size + 1\n",
    "                x_coords = np.arange(0, size, 1, np.float32)\n",
    "                y_coords = x_coords[:, np.newaxis]\n",
    "                x0 = y0 = size // 2\n",
    "                g = np.exp(- ((x_coords - x0) ** 2 + (y_coords - y0) ** 2) / (2 * self.sigma ** 2))\n",
    "                \n",
    "                g_x = max(0, -ul[0]), min(br[0], self.output_size[1]) - ul[0]\n",
    "                g_y = max(0, -ul[1]), min(br[1], self.output_size[0]) - ul[1]\n",
    "                \n",
    "                img_x = max(0, ul[0]), min(br[0], self.output_size[1])\n",
    "                img_y = max(0, ul[1]), min(br[1], self.output_size[0])\n",
    "                \n",
    "                heatmaps[i][img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
    "                \n",
    "        return torch.tensor(heatmaps, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94cb3da5-4534-42d4-9860-fb2c60d5ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # Import neural network functional API\n",
    "from torchvision.transforms import functional as TF  # Import torchvision's transforms\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "        \n",
    "\n",
    "class HighResolutionModule(nn.Module):\n",
    "    def __init__(self, branches, fusion):\n",
    "        super(HighResolutionModule, self).__init__()\n",
    "        self.branches = nn.ModuleList(branches)\n",
    "        self.fusion = nn.ModuleList(\n",
    "            [nn.ModuleList([op if op is not None else nn.Identity() for op in sublist])\n",
    "             for sublist in fusion]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process each resolution branch\n",
    "        outputs = [branch(x[i]) for i, branch in enumerate(self.branches)]\n",
    "\n",
    "        # Ensure matching spatial dimensions before fusion\n",
    "        max_height = max(output.size(2) for output in outputs)\n",
    "        max_width = max(output.size(3) for output in outputs)\n",
    "        \n",
    "        # Fuse outputs from all branches\n",
    "        for i in range(len(self.fusion)):\n",
    "            for j in range(len(outputs)):\n",
    "                if i == j: continue\n",
    "                # Apply fusion operation to outputs[j]\n",
    "                update = self.fusion[i][j](outputs[j])\n",
    "                \n",
    "                # Resize update to match the spatial size of outputs[i]\n",
    "                if update.size(2) != outputs[i].size(2) or update.size(3) != outputs[i].size(3):\n",
    "                    update = F.interpolate(update, size=(outputs[i].size(2), outputs[i].size(3)), mode='bilinear', align_corners=True)\n",
    "\n",
    "                # Add the resized update to outputs[i]\n",
    "                outputs[i] = outputs[i] + update\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class HRNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(HRNet, self).__init__()\n",
    "\n",
    "        # Stem network\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 64, 4)\n",
    "\n",
    "        # Stage 1: High-resolution branch\n",
    "        self.stage1 = self._make_layer(BasicBlock, 64, 64, 4)\n",
    "\n",
    "        # Stage 2: Multi-resolution branches\n",
    "        self.stage2 = HighResolutionModule(\n",
    "            branches=[\n",
    "                self._make_layer(BasicBlock, 64, 64, 4),\n",
    "                self._make_layer(BasicBlock, 64, 128, 4)\n",
    "            ],\n",
    "            fusion=[\n",
    "                [nn.Identity(), nn.Conv2d(128, 64, kernel_size=1, bias=False)],\n",
    "                [nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False), nn.Identity()]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Final classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(192, 17, kernel_size=1, stride=1),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride=1):\n",
    "        layers = [block(in_channels, out_channels, stride)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        # Stages\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2([x, F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=True)])\n",
    "\n",
    "        # Classifier\n",
    "        x = torch.cat([x[0], F.interpolate(x[1], scale_factor=2, mode='bilinear', align_corners=True)], dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "557c26f6-79bc-415c-8a14-651e86886622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "target_transform = HeatmapGenerator(output_size=OUTPUT_SIZE, num_keypoints=NUM_KEYPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f71de52-1f31-4a25-8c88-db12104044af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "test_dataset = PersonKeypointsDataset(\n",
    "    annotation_file=VAL_ANNOTATION_PATH,\n",
    "    image_path=VAL_IMAGE_PATH,\n",
    "    min_keypoints=9,\n",
    "    max_samples=None,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "# custom collate function to handle meta-information\n",
    "def collate_fn(batch):\n",
    "    images, targets, keypoints, metas = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    targets = torch.stack(targets, 0)\n",
    "    keypoints = torch.stack(keypoints, 0)\n",
    "    return images, targets, keypoints, metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c318790-1806-4e16-ac2e-eefa5302a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ef998-4bf4-4ce3-8e8d-0384d89470d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test_loader: 281\n",
      "\n",
      "current batch: 1\n",
      "current batch: 2\n",
      "current batch: 3\n",
      "current batch: 4\n",
      "current batch: 5\n",
      "current batch: 6\n",
      "current batch: 7\n",
      "current batch: 8\n",
      "current batch: 9\n",
      "current batch: 10\n",
      "current batch: 11\n",
      "current batch: 12\n",
      "current batch: 13\n",
      "current batch: 14\n",
      "current batch: 15\n",
      "current batch: 16\n",
      "current batch: 17\n",
      "current batch: 18\n",
      "current batch: 19\n",
      "current batch: 20\n",
      "current batch: 21\n",
      "current batch: 22\n",
      "current batch: 23\n",
      "current batch: 24\n",
      "current batch: 25\n",
      "current batch: 26\n",
      "current batch: 27\n",
      "current batch: 28\n",
      "current batch: 29\n",
      "current batch: 30\n",
      "current batch: 31\n",
      "current batch: 32\n",
      "current batch: 33\n",
      "current batch: 34\n",
      "current batch: 35\n",
      "current batch: 36\n",
      "current batch: 37\n",
      "current batch: 38\n",
      "current batch: 39\n",
      "current batch: 40\n",
      "current batch: 41\n",
      "current batch: 42\n",
      "current batch: 43\n",
      "current batch: 44\n",
      "current batch: 45\n",
      "current batch: 46\n",
      "current batch: 47\n",
      "current batch: 48\n",
      "current batch: 49\n",
      "current batch: 50\n",
      "current batch: 51\n",
      "current batch: 52\n",
      "current batch: 53\n",
      "current batch: 54\n",
      "current batch: 55\n",
      "current batch: 56\n",
      "current batch: 57\n",
      "current batch: 58\n",
      "current batch: 59\n",
      "current batch: 60\n",
      "current batch: 61\n",
      "current batch: 62\n",
      "current batch: 63\n",
      "current batch: 64\n",
      "current batch: 65\n",
      "current batch: 66\n",
      "current batch: 67\n",
      "current batch: 68\n",
      "current batch: 69\n",
      "current batch: 70\n",
      "current batch: 71\n",
      "current batch: 72\n",
      "current batch: 73\n",
      "current batch: 74\n",
      "current batch: 75\n",
      "current batch: 76\n",
      "current batch: 77\n",
      "current batch: 78\n",
      "current batch: 79\n",
      "current batch: 80\n",
      "current batch: 81\n",
      "current batch: 82\n",
      "current batch: 83\n",
      "current batch: 84\n",
      "current batch: 85\n",
      "current batch: 86\n",
      "current batch: 87\n",
      "current batch: 88\n",
      "current batch: 89\n",
      "current batch: 90\n",
      "current batch: 91\n",
      "current batch: 92\n",
      "current batch: 93\n",
      "current batch: 94\n",
      "current batch: 95\n",
      "current batch: 96\n",
      "current batch: 97\n",
      "current batch: 98\n",
      "current batch: 99\n",
      "current batch: 100\n",
      "current batch: 101\n",
      "current batch: 102\n",
      "current batch: 103\n",
      "current batch: 104\n",
      "current batch: 105\n",
      "current batch: 106\n",
      "current batch: 107\n",
      "current batch: 108\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = HRNet(num_classes=NUM_KEYPOINTS).to(device)\n",
    "model.load_state_dict(torch.load('models/HRNet_model_epoch_1_loss_0.0005904935554594125.pth', map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation loop to generate predictions\n",
    "predictions = []\n",
    "\n",
    "print(\"length of test_loader:\", len(test_loader))\n",
    "print()\n",
    "\n",
    "curr_num_batch = 1\n",
    "for batch in test_loader:\n",
    "    print(\"current batch:\", curr_num_batch)\n",
    "    curr_num_batch += 1\n",
    "    \n",
    "    images, _, _, metas = batch\n",
    "    images = images.to(device)\n",
    "    batch_size = images.size(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    \n",
    "    outputs = outputs.cpu().numpy()\n",
    "    for i in range(batch_size):\n",
    "        output_heatmaps = outputs[i]\n",
    "        meta = metas[i]\n",
    "        \n",
    "        # Extract predicted keypoints from heatmaps\n",
    "        pred_keypoints = []\n",
    "        for j in range(NUM_KEYPOINTS):\n",
    "            heatmap = output_heatmaps[j]\n",
    "            y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n",
    "            confidence = float(np.max(heatmap))\n",
    "            \n",
    "            # Map x, y back to input image size\n",
    "            pred_x = (x / OUTPUT_SIZE[1]) * INPUT_SIZE[0]\n",
    "            pred_y = (y / OUTPUT_SIZE[0]) * INPUT_SIZE[1]\n",
    "            pred_keypoints.append([pred_x, pred_y, confidence])\n",
    "        \n",
    "        pred_keypoints = np.array(pred_keypoints)\n",
    "        \n",
    "        # Reverse the resizing\n",
    "        scale_x, scale_y = meta['scale']\n",
    "        pred_keypoints[:, 0] /= scale_x\n",
    "        pred_keypoints[:, 1] /= scale_y\n",
    "        \n",
    "        # Reverse the cropping\n",
    "        x1, y1, _, _ = meta['crop_coords']\n",
    "        pred_keypoints[:, 0] += x1\n",
    "        pred_keypoints[:, 1] += y1\n",
    "        \n",
    "        # Prepare the prediction dict\n",
    "        pred_dict = {\n",
    "            'image_id': int(meta['image_id']),\n",
    "            'category_id': 1,\n",
    "            'keypoints': [],\n",
    "            'score': float(pred_keypoints[:, 2].mean())\n",
    "        }\n",
    "        \n",
    "        keypoints_list = []\n",
    "        for kp in pred_keypoints:\n",
    "            keypoints_list.extend([float(kp[0]), float(kp[1]), 1])  # Visibility flag set to 1\n",
    "        \n",
    "        pred_dict['keypoints'] = keypoints_list\n",
    "        \n",
    "        predictions.append(pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb479616-13fd-4a82-b6ce-494613102842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.10s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *keypoints*\n",
      "DONE (t=1.17s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.455\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.659\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.512\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.428\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.498\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.480\n",
      " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.662\n",
      " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.531\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.446\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.530\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# save predictions as json file\n",
    "with open('data/keypoints_predictions_hrnet.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# Load ground truth annotations\n",
    "coco_gt = COCO(VAL_ANNOTATION_PATH)\n",
    "\n",
    "# Load our predictions\n",
    "coco_dt = coco_gt.loadRes('data/keypoints_predictions_hrnet.json')\n",
    "\n",
    "# Initialize COCOeval object\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, 'keypoints')\n",
    "\n",
    "# Run evaluation\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402bd102-f97b-435c-bbed-fd80620b2ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
