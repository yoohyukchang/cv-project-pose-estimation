{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1863d-a851-47b1-8345-fde248e44211",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install Pillow\n",
    "!pip install torch torchvision\n",
    "!pip install pycocotools\n",
    "!pip install requests\n",
    "!pip install notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc56dd-0ccb-48d3-8478-9a6ada1838f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fabaef-c60d-4b38-b11d-f84f6a305c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable CuDNN benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# current path\n",
    "current_path = Path.cwd()\n",
    "\n",
    "print(\"current_path:\", current_path)\n",
    "\n",
    "# Paths to COCO dataset annotations and images\n",
    "TRAIN_ANNOTATION_PATH = current_path / \"data\" / \"person_keypoints_train2017.json\"\n",
    "VAL_ANNOTATION_PATH = current_path / \"data\" / \"person_keypoints_val2017.json\"\n",
    "TRAIN_IMAGE_PATH = current_path / \"data\" / \"train2017\"\n",
    "VAL_IMAGE_PATH = current_path / \"data\" / \"val2017\"\n",
    "\n",
    "# Number of keypoints in COCO dataset\n",
    "NUM_KEYPOINTS = 17\n",
    "\n",
    "# Input and output image sizes\n",
    "INPUT_SIZE = (192, 256)   # Width, Height\n",
    "OUTPUT_SIZE = (64, 48)    # Height, Width (for heatmaps)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff3cce-6922-4c3a-a13c-ebadf193537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303a24a-13a2-48a1-a773-22e160c65dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonKeypointsDataset(Dataset):\n",
    "    def __init__(self, annotation_file, image_path, min_keypoints=9, max_samples=None, transform=None, target_transform=None, seed=SEED):\n",
    "        \"\"\"\n",
    "        Custom Dataset for COCO Person Keypoints\n",
    "        \"\"\"\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_path = str(image_path)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.person_ids = self.coco.getCatIds(catNms=['person'])\n",
    "        self.img_ids = self.coco.getImgIds(catIds=self.person_ids)\n",
    "\n",
    "        # Set seeds for reproducibility\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # Build list of (image_id, annotation) tuples\n",
    "        self.samples = []\n",
    "        for img_id in self.img_ids:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.person_ids, iscrowd=False)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            for ann in anns:\n",
    "                # Include only samples with enough keypoints\n",
    "                if 'keypoints' in ann and ann['num_keypoints'] >= min_keypoints:\n",
    "                    self.samples.append((img_id, ann))\n",
    "\n",
    "        # Randomly sample if we have more samples than max_samples\n",
    "        if max_samples and max_samples < len(self.samples):\n",
    "            self.samples = random.sample(self.samples, max_samples)\n",
    "            print(f\"Randomly sampled {len(self.samples)} images from {len(self.samples)} total images\")\n",
    "                        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches the image and keypoints for the given index.\n",
    "        \"\"\"\n",
    "        img_id, ann = self.samples[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.image_path, img_info['file_name'])\n",
    "        if not os.path.exists(img_path):\n",
    "            raise RuntimeError(f\"Image file not found: {img_path}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get bounding box\n",
    "        bbox = ann['bbox']  # [x, y, w, h]\n",
    "        x, y, w, h = bbox\n",
    "        x1, y1 = int(x), int(y)\n",
    "        x2, y2 = int(x + w), int(y + h)\n",
    "        \n",
    "        # Crop image to bounding box\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "        \n",
    "        orig_w, orig_h = img.size\n",
    "        \n",
    "        # Adjust keypoints\n",
    "        keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "        keypoints[:, 0] -= x1\n",
    "        keypoints[:, 1] -= y1\n",
    "        \n",
    "        # Resize image and keypoints\n",
    "        new_size = INPUT_SIZE  # (width, height)\n",
    "        img = img.resize(new_size, resample=Image.BILINEAR)\n",
    "        \n",
    "        scale_x = new_size[0] / orig_w\n",
    "        scale_y = new_size[1] / orig_h\n",
    "        keypoints[:, 0] = (keypoints[:, 0] * scale_x)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] * scale_y)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = F.to_tensor(img)\n",
    "        \n",
    "        # Generate heatmaps\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(keypoints)\n",
    "        else:\n",
    "            target = torch.tensor(keypoints, dtype=torch.float32)\n",
    "        \n",
    "        return img, target, keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846d98f-d2eb-4019-9421-3b2b3620be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapGenerator:\n",
    "    \"\"\"\n",
    "    Generates heatmaps for each keypoint.\n",
    "    Uses Gaussian distributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, num_keypoints, sigma=2):\n",
    "        self.output_size = output_size  # (height, width)\n",
    "        self.num_keypoints = num_keypoints\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def __call__(self, keypoints):\n",
    "        heatmaps = np.zeros((self.num_keypoints, self.output_size[0], self.output_size[1]), dtype=np.float32)\n",
    "        tmp_size = self.sigma * 3\n",
    "        \n",
    "        for i in range(self.num_keypoints):\n",
    "            kp = keypoints[i]\n",
    "            x, y, v = kp\n",
    "            if v > 0:\n",
    "                x = x * self.output_size[1] / INPUT_SIZE[0]\n",
    "                y = y * self.output_size[0] / INPUT_SIZE[1]\n",
    "                \n",
    "                ul = [int(x - tmp_size), int(y - tmp_size)]\n",
    "                br = [int(x + tmp_size + 1), int(y + tmp_size + 1)]\n",
    "                \n",
    "                if ul[0] >= self.output_size[1] or ul[1] >= self.output_size[0] or br[0] < 0 or br[1] < 0:\n",
    "                    continue\n",
    "                \n",
    "                size = 2 * tmp_size + 1\n",
    "                x_coords = np.arange(0, size, 1, np.float32)\n",
    "                y_coords = x_coords[:, np.newaxis]\n",
    "                x0 = y0 = size // 2\n",
    "                g = np.exp(- ((x_coords - x0) ** 2 + (y_coords - y0) ** 2) / (2 * self.sigma ** 2))\n",
    "                \n",
    "                g_x = max(0, -ul[0]), min(br[0], self.output_size[1]) - ul[0]\n",
    "                g_y = max(0, -ul[1]), min(br[1], self.output_size[0]) - ul[1]\n",
    "                \n",
    "                img_x = max(0, ul[0]), min(br[0], self.output_size[1])\n",
    "                img_y = max(0, ul[1]), min(br[1], self.output_size[0])\n",
    "                \n",
    "                heatmaps[i][img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
    "                \n",
    "        return torch.tensor(heatmaps, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f2214-d71a-46d8-ab87-799d03ea6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(dataset, idx):\n",
    "    \"\"\"\n",
    "    Visualizes the image with keypoints and connections.\n",
    "    Prints the number of keypoints and edges drawn for the given image.\n",
    "    \"\"\"\n",
    "    img, _, keypoints = dataset[idx]\n",
    "    img = img.permute(1, 2, 0).numpy()  # Convert to H x W x C\n",
    "\n",
    "    plt.figure(figsize=(5, 7))\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Extract keypoint data\n",
    "    x = keypoints[:, 0]\n",
    "    y = keypoints[:, 1]\n",
    "    v = keypoints[:, 2]\n",
    "\n",
    "    # COCO keypoint connections\n",
    "    skeleton = [\n",
    "        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "        [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "        [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "        [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "    ]\n",
    "    \n",
    "    # Count visible keypoints\n",
    "    visible_keypoints = np.sum(v > 0)\n",
    "    print(f\"Image {idx}: Number of visible keypoints: {visible_keypoints}\")\n",
    "\n",
    "    # Draw limbs and count edges\n",
    "    edge_count = 0\n",
    "    for connection in skeleton:\n",
    "        p1, p2 = connection\n",
    "        if v[p1] > 0 and v[p2] > 0:  # Only connect visible keypoints\n",
    "            plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'r-', linewidth=2)\n",
    "            edge_count += 1\n",
    "\n",
    "    print(f\"Image {idx}: Number of edges drawn: {edge_count}\")\n",
    "\n",
    "    # Draw keypoints\n",
    "    for i in range(len(x)):\n",
    "        if v[i] > 0:\n",
    "            plt.plot(x[i], y[i], 'bo', markersize=5)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41695b00-6ec2-4d20-a9e7-a89293053af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize dataset\n",
    "dataset_for_visual = PersonKeypointsDataset(\n",
    "    annotation_file=TRAIN_ANNOTATION_PATH,\n",
    "    image_path=TRAIN_IMAGE_PATH,\n",
    "    min_keypoints=9,\n",
    "    max_samples=None,\n",
    "    transform=transform,\n",
    "    target_transform=None  # no need to use heatmaps for visualization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e82f9-e707-496e-9a35-485a348a0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_for_visual))\n",
    "\n",
    "# Visualize a few samples\n",
    "for idx in range(100, 110):\n",
    "    visualize_sample(dataset_for_visual, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01228e33-80c5-4d2a-aaa1-27eea83e94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualize Heatmaps and Keypoints ####\n",
    "\n",
    "def visualize_with_keypoints_and_heatmaps(dataset, idx, heatmap_generator):\n",
    "    \"\"\"\n",
    "    Visualize the original image and overlay heatmap with keypoints and skeleton connections.\n",
    "    \"\"\"\n",
    "    img, _, keypoints = dataset[idx]\n",
    "    img = img.permute(1, 2, 0).numpy()  # Convert to H x W x C for visualization\n",
    "\n",
    "    # Generate heatmaps\n",
    "    heatmaps = heatmap_generator(keypoints)\n",
    "    heatmaps_np = heatmaps.numpy()\n",
    "    combined_heatmap = np.sum(heatmaps_np, axis=0)\n",
    "\n",
    "    # Extract keypoint data\n",
    "    x = keypoints[:, 0]\n",
    "    y = keypoints[:, 1]\n",
    "    v = keypoints[:, 2]\n",
    "\n",
    "    # COCO keypoint connections\n",
    "    skeleton = [\n",
    "        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "        [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "        [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "        [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "    ]\n",
    "\n",
    "    # Plot original image on the left\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot overlay heatmap with keypoints and skeleton on the right\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img, alpha=0.5)\n",
    "    plt.imshow(combined_heatmap, cmap='hot', alpha=0.5)\n",
    "    \n",
    "    # Draw limbs and count edges\n",
    "    edge_count = 0\n",
    "    for connection in skeleton:\n",
    "        p1, p2 = connection\n",
    "        if v[p1] > 0 and v[p2] > 0:  # Only connect visible keypoints\n",
    "            plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'r-', linewidth=2)\n",
    "            edge_count += 1\n",
    "\n",
    "    # Draw keypoints\n",
    "    for i in range(len(x)):\n",
    "        if v[i] > 0:\n",
    "            plt.plot(x[i], y[i], 'bo', markersize=5)\n",
    "\n",
    "    plt.title(\"Overlay Heatmap with Keypoints\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create an instance of HeatmapGenerator\n",
    "heatmap_generator = HeatmapGenerator(output_size=OUTPUT_SIZE, num_keypoints=NUM_KEYPOINTS)\n",
    "\n",
    "# Visualize heatmaps and keypoints for random samples\n",
    "for _ in range(10):\n",
    "    random_idx = random.randint(0, len(dataset_for_visual) - 1)\n",
    "    visualize_with_keypoints_and_heatmaps(dataset_for_visual, random_idx, heatmap_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa6b657-b6c7-483c-9c62-c17c1326ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaPoseModel(nn.Module):\n",
    "    def __init__(self, num_keypoints):\n",
    "        super(AlphaPoseModel, self).__init__()\n",
    "        # Load pretrained ResNet50\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        # Remove the last fc layer and avgpool\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        # Deconvolution layers\n",
    "        self.deconv_layers = self._make_deconv_layer(3, [256, 256, 256], [4, 4, 4])\n",
    "        # Final layer to predict heatmaps\n",
    "        self.final_layer = nn.Conv2d(in_channels=256, out_channels=num_keypoints, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def _make_deconv_layer(self, num_layers, num_filters, num_kernels):\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            kernel = num_kernels[i]\n",
    "            planes = num_filters[i]\n",
    "            layers.append(nn.ConvTranspose2d(\n",
    "                in_channels=2048 if i==0 else num_filters[i-1],\n",
    "                out_channels=planes,\n",
    "                kernel_size=kernel,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=0,\n",
    "                bias=False))\n",
    "            layers.append(nn.BatchNorm2d(planes))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.deconv_layers(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5977440-e3a7-4142-a40f-361f0c106296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize dataset with target_transform (heatmaps)\n",
    "target_transform = HeatmapGenerator(output_size=OUTPUT_SIZE, num_keypoints=NUM_KEYPOINTS)\n",
    "\n",
    "dataset = PersonKeypointsDataset(\n",
    "    annotation_file=TRAIN_ANNOTATION_PATH,\n",
    "    image_path=TRAIN_IMAGE_PATH,\n",
    "    min_keypoints=9,\n",
    "    max_samples=None,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Initialize test dataset (using COCO validation data)\n",
    "test_dataset = PersonKeypointsDataset(\n",
    "    annotation_file=VAL_ANNOTATION_PATH,\n",
    "    image_path=VAL_IMAGE_PATH,\n",
    "    min_keypoints=9,\n",
    "    max_samples=None,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ce75e-b8a0-413e-9438-4101ff80f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust batch size based on GPU memory\n",
    "BATCH_SIZE = 512  # Increase batch size to utilize GPU memory\n",
    "\n",
    "# Initialize DataLoaders with more workers\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=16, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=16, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765583df-e35c-47fd-a479-f685ca12ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = AlphaPoseModel(num_keypoints=NUM_KEYPOINTS).to(device)\n",
    "\n",
    "# Compile the model if using PyTorch 2.0 or newer\n",
    "try:\n",
    "    model = torch.compile(model)\n",
    "except:\n",
    "    print(\"torch.compile is not available. Continuing without model compilation.\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649fd607-c095-4206-95df-5effd93f77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = GradScaler('cuda' if device.type == 'cuda' else None)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training loop\n",
    "    for images, targets, _ in train_loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training: forward pass\n",
    "        if device.type == 'cuda':\n",
    "            with autocast('cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets, _ in val_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            # Mixed precision validation: forward pass\n",
    "            if device.type == 'cuda':\n",
    "                with autocast('cuda'):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, targets)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Time: {elapsed_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53decf-80c4-40b8-8841-68736e81831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1, NUM_EPOCHS+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4994708-a510-4a9b-8146-fb6c0ce27b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, dataset, idx):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img, _, keypoints = dataset[idx]\n",
    "        img_tensor = img.unsqueeze(0).to(device)\n",
    "        outputs = model(img_tensor)\n",
    "        output_heatmaps = outputs.squeeze(0).cpu()\n",
    "        \n",
    "        # Get predicted keypoints from heatmaps\n",
    "        pred_keypoints = []\n",
    "        for i in range(NUM_KEYPOINTS):\n",
    "            heatmap = output_heatmaps[i]\n",
    "            y, x = np.unravel_index(heatmap.argmax(), heatmap.shape)\n",
    "            confidence = heatmap.max()\n",
    "            pred_x = (x / OUTPUT_SIZE[1]) * INPUT_SIZE[0]\n",
    "            pred_y = (y / OUTPUT_SIZE[0]) * INPUT_SIZE[1]\n",
    "            pred_keypoints.append((pred_x, pred_y, confidence))\n",
    "        pred_keypoints = np.array(pred_keypoints)\n",
    "        \n",
    "        # Visualize Ground Truth\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_np)\n",
    "        x = keypoints[:, 0]\n",
    "        y = keypoints[:, 1]\n",
    "        v = keypoints[:, 2]\n",
    "\n",
    "        # Draw ground truth skeleton\n",
    "        for connection in skeleton:\n",
    "            p1, p2 = connection\n",
    "            if v[p1] > 0 and v[p2] > 0:\n",
    "                plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'r-', linewidth=2)\n",
    "        for i in range(len(x)):\n",
    "            if v[i] > 0:\n",
    "                plt.plot(x[i], y[i], 'bo', markersize=5)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Visualize Prediction\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(img_np)\n",
    "        x = pred_keypoints[:, 0]\n",
    "        y = pred_keypoints[:, 1]\n",
    "        c = pred_keypoints[:, 2]\n",
    "\n",
    "        # Draw predicted skeleton\n",
    "        for connection in skeleton:\n",
    "            p1, p2 = connection\n",
    "            if c[p1] > 0.1 and c[p2] > 0.1:  # Threshold confidence\n",
    "                plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'g-', linewidth=2)\n",
    "        for i in range(len(x)):\n",
    "            if c[i] > 0.1:\n",
    "                plt.plot(x[i], y[i], 'yo', markersize=5)\n",
    "        plt.title('Model Prediction')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Visualize results on test data\n",
    "skeleton = [\n",
    "    [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "    [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "    [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "    [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "]\n",
    "skeleton = [[p1, p2] for p1, p2 in skeleton]\n",
    "\n",
    "# Visualize model predictions on test dataset\n",
    "for i in range(100, 150):\n",
    "    visualize_results(model, test_dataset, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbc834-6862-4a6e-bf5d-25caa1e2d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'AlphaPose_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
