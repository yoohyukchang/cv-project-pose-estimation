{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b13d49d-b37e-4077-973f-11501b1c934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff99ee2-5ff0-47d5-9ab2-f46b77c4f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current path\n",
    "current_path = Path.cwd()\n",
    "\n",
    "# Paths to COCO dataset annotations and images\n",
    "TRAIN_ANNOTATION_PATH = current_path / \"data\" / \"person_keypoints_train2017.json\"\n",
    "VAL_ANNOTATION_PATH = current_path / \"data\" / \"person_keypoints_val2017.json\"\n",
    "TRAIN_IMAGE_PATH = \"http://images.cocodataset.org/train2017/\"\n",
    "VAL_IMAGE_PATH = \"http://images.cocodataset.org/val2017/\"\n",
    "\n",
    "# Number of keypoints in COCO dataset\n",
    "NUM_KEYPOINTS = 17\n",
    "\n",
    "# Input and output image sizes\n",
    "INPUT_SIZE = (192, 256)   # Width, Height\n",
    "OUTPUT_SIZE = (64, 48)    # Height, Width (for heatmaps)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d52c31-219d-413a-899d-eb53a856a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d94dcb-cbd8-4ec8-b3c7-5735204b40ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonKeypointsDataset(Dataset):\n",
    "    def __init__(self, annotation_file, image_path, min_keypoints=9, max_samples=None, transform=None, target_transform=None, seed=SEED):\n",
    "        \"\"\"\n",
    "        Custom Dataset for COCO Person Keypoints\n",
    "        \"\"\"\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_path = image_path\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.person_ids = self.coco.getCatIds(catNms=['person'])\n",
    "        self.img_ids = self.coco.getImgIds(catIds=self.person_ids)\n",
    "\n",
    "        # Set seeds for reproducibility\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # Build list of (image_id, annotation) tuples\n",
    "        self.samples = []\n",
    "        for img_id in self.img_ids:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.person_ids, iscrowd=False)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            for ann in anns:\n",
    "                # Include only samples with enough keypoints\n",
    "                if 'keypoints' in ann and ann['num_keypoints'] >= min_keypoints:\n",
    "                    self.samples.append((img_id, ann))\n",
    "\n",
    "        # Randomly sample if we have more samples than max_samples\n",
    "        if max_samples and max_samples < len(self.samples):\n",
    "            self.samples = random.sample(self.samples, max_samples)\n",
    "            print(f\"Randomly sampled {len(self.samples)} images from {len(self.samples)} total images\")\n",
    "                        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches the image and keypoints for the given index.\n",
    "        \"\"\"\n",
    "        img_id, ann = self.samples[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_url = self.image_path + img_info['file_name']\n",
    "        response = requests.get(img_url)\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(f\"Failed to download image: {img_url}\")\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        \n",
    "        # Get bounding box\n",
    "        bbox = ann['bbox']  # [x, y, w, h]\n",
    "        x, y, w, h = bbox\n",
    "        x1, y1 = int(x), int(y)\n",
    "        x2, y2 = int(x + w), int(y + h)\n",
    "        \n",
    "        # Crop image to bounding box\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "        \n",
    "        orig_w, orig_h = img.size\n",
    "        \n",
    "        # Adjust keypoints\n",
    "        keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "        keypoints[:, 0] -= x1\n",
    "        keypoints[:, 1] -= y1\n",
    "        \n",
    "        # Resize image and keypoints\n",
    "        new_size = INPUT_SIZE  # (width, height)\n",
    "        img = img.resize(new_size, resample=Image.BILINEAR)\n",
    "        \n",
    "        scale_x = new_size[0] / orig_w\n",
    "        scale_y = new_size[1] / orig_h\n",
    "        keypoints[:, 0] = (keypoints[:, 0] * scale_x)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] * scale_y)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = F.to_tensor(img)\n",
    "        \n",
    "        # Generate heatmaps\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(keypoints)\n",
    "        else:\n",
    "            target = torch.tensor(keypoints, dtype=torch.float32)\n",
    "        \n",
    "        return img, target, keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81631574-5abb-40f1-ace0-0d34331147af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapGenerator:\n",
    "    \"\"\"\n",
    "    Generates heatmaps for each keypoint.\n",
    "    Uses Gaussian distributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, num_keypoints, sigma=2):\n",
    "        self.output_size = output_size  # (height, width)\n",
    "        self.num_keypoints = num_keypoints\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def __call__(self, keypoints):\n",
    "        heatmaps = np.zeros((self.num_keypoints, self.output_size[0], self.output_size[1]), dtype=np.float32)\n",
    "        tmp_size = self.sigma * 3\n",
    "        \n",
    "        for i in range(self.num_keypoints):\n",
    "            kp = keypoints[i]\n",
    "            x, y, v = kp\n",
    "            if v > 0:\n",
    "                x = x * self.output_size[1] / INPUT_SIZE[0]\n",
    "                y = y * self.output_size[0] / INPUT_SIZE[1]\n",
    "                \n",
    "                ul = [int(x - tmp_size), int(y - tmp_size)]\n",
    "                br = [int(x + tmp_size + 1), int(y + tmp_size + 1)]\n",
    "                \n",
    "                if ul[0] >= self.output_size[1] or ul[1] >= self.output_size[0] or br[0] < 0 or br[1] < 0:\n",
    "                    continue\n",
    "                \n",
    "                size = 2 * tmp_size + 1\n",
    "                x_coords = np.arange(0, size, 1, np.float32)\n",
    "                y_coords = x_coords[:, np.newaxis]\n",
    "                x0 = y0 = size // 2\n",
    "                g = np.exp(- ((x_coords - x0) ** 2 + (y_coords - y0) ** 2) / (2 * self.sigma ** 2))\n",
    "                \n",
    "                g_x = max(0, -ul[0]), min(br[0], self.output_size[1]) - ul[0]\n",
    "                g_y = max(0, -ul[1]), min(br[1], self.output_size[0]) - ul[1]\n",
    "                \n",
    "                img_x = max(0, ul[0]), min(br[0], self.output_size[1])\n",
    "                img_y = max(0, ul[1]), min(br[1], self.output_size[0])\n",
    "                \n",
    "                heatmaps[i][img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
    "                \n",
    "        return torch.tensor(heatmaps, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a464eb-370a-4f76-9d77-20112b78505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(dataset, idx):\n",
    "    \"\"\"\n",
    "    Visualizes the image with keypoints and connections.\n",
    "    Prints the number of keypoints and edges drawn for the given image.\n",
    "    \"\"\"\n",
    "    img, _, keypoints = dataset[idx]\n",
    "    img = img.permute(1, 2, 0).numpy()  # Convert to H x W x C\n",
    "\n",
    "    plt.figure(figsize=(5, 7))\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Extract keypoint data\n",
    "    x = keypoints[:, 0]\n",
    "    y = keypoints[:, 1]\n",
    "    v = keypoints[:, 2]\n",
    "\n",
    "    # COCO keypoint connections\n",
    "    skeleton = [\n",
    "        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "        [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "        [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "        [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "    ]\n",
    "    \n",
    "    # Count visible keypoints\n",
    "    visible_keypoints = np.sum(v > 0)\n",
    "    print(f\"Image {idx}: Number of visible keypoints: {visible_keypoints}\")\n",
    "\n",
    "    # Draw limbs and count edges\n",
    "    edge_count = 0\n",
    "    for connection in skeleton:\n",
    "        p1, p2 = connection\n",
    "        if v[p1] > 0 and v[p2] > 0:  # Only connect visible keypoints\n",
    "            plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'r-', linewidth=2)\n",
    "            edge_count += 1\n",
    "\n",
    "    print(f\"Image {idx}: Number of edges drawn: {edge_count}\")\n",
    "\n",
    "    # Draw keypoints\n",
    "    for i in range(len(x)):\n",
    "        if v[i] > 0:\n",
    "            plt.plot(x[i], y[i], 'bo', markersize=5)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7003df21-910e-4ce9-9ba0-cf97f41c479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize dataset\n",
    "dataset_for_visual = PersonKeypointsDataset(\n",
    "    annotation_file=TRAIN_ANNOTATION_PATH,\n",
    "    image_path=TRAIN_IMAGE_PATH,\n",
    "    min_keypoints=9,\n",
    "    max_samples=200,\n",
    "    transform=transform,\n",
    "    target_transform=None  # no need to use heatmaps for visualization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18906322-c8c6-4d6d-83e7-318555ca1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few samples\n",
    "for idx in range(100, 110):\n",
    "    visualize_sample(dataset_for_visual, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a586d6-bdb0-4347-a8d2-08b791550be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualize Heatmaps and Keypoints ####\n",
    "\n",
    "def visualize_with_keypoints_and_heatmaps(dataset, idx, heatmap_generator):\n",
    "    \"\"\"\n",
    "    Visualize the original image and overlay heatmap with keypoints and skeleton connections.\n",
    "    \"\"\"\n",
    "    img, _, keypoints = dataset[idx]\n",
    "    img = img.permute(1, 2, 0).numpy()  # Convert to H x W x C for visualization\n",
    "\n",
    "    # Generate heatmaps\n",
    "    heatmaps = heatmap_generator(keypoints)\n",
    "    heatmaps_np = heatmaps.numpy()\n",
    "    combined_heatmap = np.sum(heatmaps_np, axis=0)\n",
    "\n",
    "    # Extract keypoint data\n",
    "    x = keypoints[:, 0]\n",
    "    y = keypoints[:, 1]\n",
    "    v = keypoints[:, 2]\n",
    "\n",
    "    # COCO keypoint connections\n",
    "    skeleton = [\n",
    "        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "        [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "        [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "        [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "    ]\n",
    "\n",
    "    # Plot original image on the left\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot overlay heatmap with keypoints and skeleton on the right\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img, alpha=0.5)\n",
    "    plt.imshow(combined_heatmap, cmap='hot', alpha=0.5)\n",
    "    \n",
    "    # Draw limbs and count edges\n",
    "    edge_count = 0\n",
    "    for connection in skeleton:\n",
    "        p1, p2 = connection\n",
    "        if v[p1] > 0 and v[p2] > 0:  # Only connect visible keypoints\n",
    "            plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'r-', linewidth=2)\n",
    "            edge_count += 1\n",
    "\n",
    "    # Draw keypoints\n",
    "    for i in range(len(x)):\n",
    "        if v[i] > 0:\n",
    "            plt.plot(x[i], y[i], 'bo', markersize=5)\n",
    "\n",
    "    plt.title(\"Overlay Heatmap with Keypoints\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize heatmaps and keypoints for a random sample\n",
    "for _ in range(10):\n",
    "    random_idx = random.randint(0, len(dataset_for_visual) - 1)\n",
    "    visualize_with_keypoints_and_heatmaps(dataset_for_visual, random_idx, target_transform)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba5ddb-f24d-433b-a11e-088534646683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaPoseModel(nn.Module):\n",
    "    def __init__(self, num_keypoints):\n",
    "        super(AlphaPoseModel, self).__init__()\n",
    "        # Load pretrained ResNet50\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        # Remove the last fc layer and avgpool\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        # Deconvolution layers\n",
    "        self.deconv_layers = self._make_deconv_layer(3, [256, 256, 256], [4, 4, 4])\n",
    "        # Final layer to predict heatmaps\n",
    "        self.final_layer = nn.Conv2d(in_channels=256, out_channels=num_keypoints, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def _make_deconv_layer(self, num_layers, num_filters, num_kernels):\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            kernel = num_kernels[i]\n",
    "            planes = num_filters[i]\n",
    "            layers.append(nn.ConvTranspose2d(\n",
    "                in_channels=2048 if i==0 else num_filters[i-1],\n",
    "                out_channels=planes,\n",
    "                kernel_size=kernel,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=0,\n",
    "                bias=False))\n",
    "            layers.append(nn.BatchNorm2d(planes))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.deconv_layers(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89f34b-e7e3-41b0-9e0a-501ed947a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize dataset with target_transform (heatmaps)\n",
    "target_transform = HeatmapGenerator(output_size=OUTPUT_SIZE, num_keypoints=NUM_KEYPOINTS)\n",
    "\n",
    "dataset = PersonKeypointsDataset(\n",
    "    annotation_file=TRAIN_ANNOTATION_PATH,\n",
    "    image_path=TRAIN_IMAGE_PATH,\n",
    "    min_keypoints=9,\n",
    "    max_samples=2000,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Initialize test dataset (using COCO validation data)\n",
    "test_dataset = PersonKeypointsDataset(\n",
    "    annotation_file=VAL_ANNOTATION_PATH,\n",
    "    image_path=VAL_IMAGE_PATH,\n",
    "    min_keypoints=9,\n",
    "    max_samples=150,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00871e5-a36d-4e88-9d47-0f41feb7889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16 # (make it to be 16 later for actual training)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722df81-f49e-4340-85c5-8aa32bb81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = AlphaPoseModel(num_keypoints=NUM_KEYPOINTS).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ae396-6328-41ef-8d84-0dcdb855f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    for images, targets, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9b449-7d06-4246-9a2b-676813a0ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e7186-5634-4130-b059-c564e16fae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, dataset, idx):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img, _, keypoints = dataset[idx]\n",
    "        img_tensor = img.unsqueeze(0).to(device)\n",
    "        outputs = model(img_tensor)\n",
    "        output_heatmaps = outputs.squeeze(0).cpu()\n",
    "        \n",
    "        # Get predicted keypoints from heatmaps\n",
    "        pred_keypoints = []\n",
    "        for i in range(NUM_KEYPOINTS):\n",
    "            heatmap = output_heatmaps[i]\n",
    "            y, x = np.unravel_index(heatmap.argmax(), heatmap.shape)\n",
    "            confidence = heatmap.max()\n",
    "            pred_x = (x / OUTPUT_SIZE[1]) * INPUT_SIZE[0]\n",
    "            pred_y = (y / OUTPUT_SIZE[0]) * INPUT_SIZE[1]\n",
    "            pred_keypoints.append((pred_x, pred_y, confidence))\n",
    "        pred_keypoints = np.array(pred_keypoints)\n",
    "        \n",
    "        # Visualize Ground Truth\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_np)\n",
    "        x = keypoints[:, 0]\n",
    "        y = keypoints[:, 1]\n",
    "        v = keypoints[:, 2]\n",
    "\n",
    "        # Draw ground truth skeleton\n",
    "        for connection in skeleton:\n",
    "            p1, p2 = connection\n",
    "            if v[p1] > 0 and v[p2] > 0:\n",
    "                plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'r-', linewidth=2)\n",
    "        for i in range(len(x)):\n",
    "            if v[i] > 0:\n",
    "                plt.plot(x[i], y[i], 'bo', markersize=5)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Visualize Prediction\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(img_np)\n",
    "        x = pred_keypoints[:, 0]\n",
    "        y = pred_keypoints[:, 1]\n",
    "        c = pred_keypoints[:, 2]\n",
    "\n",
    "        # Draw predicted skeleton\n",
    "        for connection in skeleton:\n",
    "            p1, p2 = connection\n",
    "            if c[p1] > 0.1 and c[p2] > 0.1:  # Threshold confidence\n",
    "                plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'g-', linewidth=2)\n",
    "        for i in range(len(x)):\n",
    "            if c[i] > 0.1:\n",
    "                plt.plot(x[i], y[i], 'yo', markersize=5)\n",
    "        plt.title('Model Prediction')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Visualize results on test data\n",
    "skeleton = [\n",
    "    [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "    [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "    [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "    [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "]\n",
    "skeleton = [[p1, p2] for p1, p2 in skeleton]\n",
    "\n",
    "# Choose an index from test dataset\n",
    "test_idx = 3\n",
    "\n",
    "# visualize_results(model, test_dataset, test_idx)\n",
    "for i in range(100, 150):\n",
    "    visualize_results(model, test_dataset, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537b0b5-0ade-4e00-b0bf-2ccddc89f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'models/AlphaPose_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4fec81-a24c-4335-9fdf-845660572831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
