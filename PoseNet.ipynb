{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985c0ac-6a91-458e-a6c1-cb0341689b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33bd05-f634-4c2a-b79d-bc26051b1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to COCO dataset annotations and images\n",
    "TRAIN_ANNOTATION_PATH = \"data/person_keypoints_train2017.json\"\n",
    "VAL_ANNOTATION_PATH = \"data/person_keypoints_val2017.json\"\n",
    "TRAIN_IMAGE_PATH = \"http://images.cocodataset.org/train2017/\"\n",
    "VAL_IMAGE_PATH = \"http://images.cocodataset.org/val2017/\"\n",
    "\n",
    "# Number of keypoints in COCO dataset\n",
    "NUM_KEYPOINTS = 17\n",
    "\n",
    "# Input and output image sizes\n",
    "INPUT_SIZE = (192, 256)   # Width, Height\n",
    "OUTPUT_SIZE = (64, 48)    # Height, Width (for heatmaps)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785eb1f-97f8-4d49-8deb-acecd0b35c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c96ea-4701-4c34-bbde-07f2a91eda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonKeypointsDataset(Dataset):\n",
    "    def __init__(self, annotation_file, image_path, min_keypoints=9, max_samples=None, transform=None, target_transform=None, seed=SEED):\n",
    "        \"\"\"\n",
    "        Custom Dataset for COCO Person Keypoints\n",
    "        \"\"\"\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_path = image_path\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.person_ids = self.coco.getCatIds(catNms=['person'])\n",
    "        self.img_ids = self.coco.getImgIds(catIds=self.person_ids)\n",
    "\n",
    "        # Set seeds for reproducibility\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # Build list of (image_id, annotation) tuples\n",
    "        self.samples = []\n",
    "        for img_id in self.img_ids:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.person_ids, iscrowd=False)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            for ann in anns:\n",
    "                # Include only samples with enough keypoints\n",
    "                if 'keypoints' in ann and ann['num_keypoints'] >= min_keypoints:\n",
    "                    self.samples.append((img_id, ann))\n",
    "\n",
    "        # Randomly sample if we have more samples than max_samples\n",
    "        if max_samples and max_samples < len(self.samples):\n",
    "            self.samples = random.sample(self.samples, max_samples)\n",
    "            print(f\"Randomly sampled {len(self.samples)} images from {len(self.samples)} total images\")\n",
    "                        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches the image and keypoints for the given index.\n",
    "        \"\"\"\n",
    "        img_id, ann = self.samples[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_url = self.image_path + img_info['file_name']\n",
    "        response = requests.get(img_url)\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(f\"Failed to download image: {img_url}\")\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        \n",
    "        # Get bounding box\n",
    "        bbox = ann['bbox']  # [x, y, w, h]\n",
    "        x, y, w, h = bbox\n",
    "        x1, y1 = int(x), int(y)\n",
    "        x2, y2 = int(x + w), int(y + h)\n",
    "        \n",
    "        # Crop image to bounding box\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "        \n",
    "        orig_w, orig_h = img.size\n",
    "        \n",
    "        # Adjust keypoints\n",
    "        keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "        keypoints[:, 0] -= x1\n",
    "        keypoints[:, 1] -= y1\n",
    "        \n",
    "        # Resize image and keypoints\n",
    "        new_size = INPUT_SIZE  # (width, height)\n",
    "        img = img.resize(new_size, resample=Image.BILINEAR)\n",
    "        \n",
    "        scale_x = new_size[0] / orig_w\n",
    "        scale_y = new_size[1] / orig_h\n",
    "        keypoints[:, 0] = (keypoints[:, 0] * scale_x)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] * scale_y)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = F.to_tensor(img)\n",
    "        \n",
    "        # Generate heatmaps\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(keypoints)\n",
    "        else:\n",
    "            target = torch.tensor(keypoints, dtype=torch.float32)\n",
    "        \n",
    "        return img, target, keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2fa13-6063-411e-a1ce-eb1c5915214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapGenerator:\n",
    "    \"\"\"\n",
    "    Generates heatmaps for each keypoint.\n",
    "    Uses Gaussian distributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, num_keypoints, sigma=2):\n",
    "        self.output_size = output_size  # (height, width)\n",
    "        self.num_keypoints = num_keypoints\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def __call__(self, keypoints):\n",
    "        heatmaps = np.zeros((self.num_keypoints, self.output_size[0], self.output_size[1]), dtype=np.float32)\n",
    "        tmp_size = self.sigma * 3\n",
    "\n",
    "        for i in range(self.num_keypoints):\n",
    "            kp = keypoints[i]\n",
    "            x, y, v = kp\n",
    "            if v > 0:\n",
    "                x = x * self.output_size[1] / INPUT_SIZE[0]\n",
    "                y = y * self.output_size[0] / INPUT_SIZE[1]\n",
    "                \n",
    "                ul = [int(x - tmp_size), int(y - tmp_size)]\n",
    "                br = [int(x + tmp_size + 1), int(y + tmp_size + 1)]\n",
    "                \n",
    "                if ul[0] >= self.output_size[1] or ul[1] >= self.output_size[0] or br[0] < 0 or br[1] < 0:\n",
    "                    continue\n",
    "                \n",
    "                size = 2 * tmp_size + 1\n",
    "                x_coords = np.arange(0, size, 1, np.float32)\n",
    "                y_coords = x_coords[:, np.newaxis]\n",
    "                x0 = y0 = size // 2\n",
    "                g = np.exp(- ((x_coords - x0) ** 2 + (y_coords - y0) ** 2) / (2 * self.sigma ** 2))\n",
    "                \n",
    "                g_x = max(0, -ul[0]), min(br[0], self.output_size[1]) - ul[0]\n",
    "                g_y = max(0, -ul[1]), min(br[1], self.output_size[0]) - ul[1]\n",
    "                \n",
    "                img_x = max(0, ul[0]), min(br[0], self.output_size[1])\n",
    "                img_y = max(0, ul[1]), min(br[1], self.output_size[0])\n",
    "                \n",
    "                heatmaps[i][img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n",
    "                \n",
    "        return torch.tensor(heatmaps, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fccb47-1fdf-4ed3-8273-b483ba9b6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(dataset, idx):\n",
    "    \"\"\"\n",
    "    Visualizes the image with keypoints and connections.\n",
    "    Prints the number of keypoints and edges drawn for the given image.\n",
    "    \"\"\"\n",
    "    img, _, keypoints = dataset[idx]\n",
    "    img = img.permute(1, 2, 0).numpy()  # Convert to H x W x C\n",
    "\n",
    "    plt.figure(figsize=(5, 7))\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Extract keypoint data\n",
    "    x = keypoints[:, 0]\n",
    "    y = keypoints[:, 1]\n",
    "    v = keypoints[:, 2]\n",
    "\n",
    "    # COCO keypoint connections\n",
    "    skeleton = [\n",
    "        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "        [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "        [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "        [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "    ]\n",
    "    \n",
    "    # Count visible keypoints\n",
    "    visible_keypoints = np.sum(v > 0)\n",
    "    print(f\"Image {idx}: Number of visible keypoints: {visible_keypoints}\")\n",
    "\n",
    "    # Draw limbs and count edges\n",
    "    edge_count = 0\n",
    "    for connection in skeleton:\n",
    "        p1, p2 = connection\n",
    "        if v[p1] > 0 and v[p2] > 0:  # Only connect visible keypoints\n",
    "            plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'r-', linewidth=2)\n",
    "            edge_count += 1\n",
    "\n",
    "    print(f\"Image {idx}: Number of edges drawn: {edge_count}\")\n",
    "\n",
    "    # Draw keypoints\n",
    "    for i in range(len(x)):\n",
    "        if v[i] > 0:\n",
    "            plt.plot(x[i], y[i], 'bo', markersize=5)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7595c-7e26-4886-9dd9-c9942d0ef577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize dataset\n",
    "dataset_for_visual = PersonKeypointsDataset(\n",
    "    annotation_file=TRAIN_ANNOTATION_PATH,\n",
    "    image_path=TRAIN_IMAGE_PATH,\n",
    "    min_keypoints=9,\n",
    "    max_samples=200,\n",
    "    transform=transform,\n",
    "    target_transform=None  # no need to use heatmaps for visualization\n",
    ")\n",
    "\n",
    "# Visualize a few samples\n",
    "for idx in range(5):\n",
    "    visualize_sample(dataset_for_visual, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8044ca1-f15a-4aa4-9d90-cb5098347cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_with_keypoints_and_heatmaps(dataset, idx, heatmap_generator):\n",
    "    \"\"\"\n",
    "    Visualize the original image and overlay heatmap with keypoints and skeleton connections.\n",
    "    \"\"\"\n",
    "    img, _, keypoints = dataset[idx]\n",
    "    img = img.permute(1, 2, 0).numpy()  # Convert to H x W x C for visualization\n",
    "\n",
    "    # Generate heatmaps\n",
    "    heatmaps = heatmap_generator(keypoints)\n",
    "    heatmaps_np = heatmaps.numpy()\n",
    "    combined_heatmap = np.sum(heatmaps_np, axis=0)\n",
    "\n",
    "    # Extract keypoint data\n",
    "    x = keypoints[:, 0]\n",
    "    y = keypoints[:, 1]\n",
    "    v = keypoints[:, 2]\n",
    "\n",
    "    # COCO keypoint connections\n",
    "    skeleton = [\n",
    "        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "        [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "        [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "        [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "    ]\n",
    "\n",
    "    # Plot original image on the left\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot overlay heatmap with keypoints and skeleton on the right\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img, alpha=0.5)\n",
    "    plt.imshow(combined_heatmap, cmap='hot', alpha=0.5)\n",
    "    \n",
    "    # Draw limbs\n",
    "    for connection in skeleton:\n",
    "        p1, p2 = connection\n",
    "        if v[p1] > 0 and v[p2] > 0:  # Only connect visible keypoints\n",
    "            plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'r-', linewidth=2)\n",
    "\n",
    "    # Draw keypoints\n",
    "    for i in range(len(x)):\n",
    "        if v[i] > 0:\n",
    "            plt.plot(x[i], y[i], 'bo', markersize=5)\n",
    "\n",
    "    plt.title(\"Overlay Heatmap with Keypoints\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48519fb1-9d76-4642-8565-c64a21413ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Heatmap Generator\n",
    "heatmap_generator = HeatmapGenerator(output_size=OUTPUT_SIZE, num_keypoints=NUM_KEYPOINTS)\n",
    "\n",
    "# Visualize heatmaps and keypoints for a few samples\n",
    "for idx in range(5):\n",
    "    visualize_with_keypoints_and_heatmaps(dataset_for_visual, idx, heatmap_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81039837-e74b-4eb8-ac3e-4179fe1b3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class PoseNet(nn.Module):\n",
    "    def __init__(self, num_keypoints):\n",
    "        super(PoseNet, self).__init__()\n",
    "        # Load pretrained ResNet50 as backbone\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove the last two layers (avgpool and fc)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        \n",
    "        # Refinement stages\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ConvBlock(2048, 256),\n",
    "            ConvBlock(256, 256),\n",
    "            ConvBlock(256, 256)\n",
    "        )\n",
    "        \n",
    "        self.stage2 = nn.Sequential(\n",
    "            ConvBlock(256, 128),\n",
    "            ConvBlock(128, 128),\n",
    "            ConvBlock(128, 128)\n",
    "        )\n",
    "        \n",
    "        self.stage3 = nn.Sequential(\n",
    "            ConvBlock(128, 64),\n",
    "            ConvBlock(64, 64),\n",
    "            ConvBlock(64, 64)\n",
    "        )\n",
    "        \n",
    "        # Upsampling layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        # Final layers for keypoint prediction\n",
    "        self.final_stage = nn.Sequential(\n",
    "            ConvBlock(64, 32),\n",
    "            nn.Conv2d(32, num_keypoints, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Backbone feature extraction\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Refinement stages with upsampling\n",
    "        x = self.stage1(x)\n",
    "        x = self.deconv1(x)\n",
    "        \n",
    "        x = self.stage2(x)\n",
    "        x = self.deconv2(x)\n",
    "        \n",
    "        x = self.stage3(x)\n",
    "        x = self.deconv3(x)\n",
    "        \n",
    "        # Final prediction\n",
    "        x = self.final_stage(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7cb8ab-350c-4078-9bc8-dfae1853b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize dataset with target_transform (heatmaps)\n",
    "target_transform = HeatmapGenerator(output_size=OUTPUT_SIZE, num_keypoints=NUM_KEYPOINTS)\n",
    "\n",
    "dataset = PersonKeypointsDataset(\n",
    "    annotation_file=TRAIN_ANNOTATION_PATH,\n",
    "    image_path=TRAIN_IMAGE_PATH,\n",
    "    min_keypoints=9,\n",
    "    max_samples=100,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496fa20-eb73-4597-90a3-33e112cf3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16  # Adjust based on your GPU memory\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0014e7-ad56-4622-9595-1ace21e46fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = PoseNet(num_keypoints=NUM_KEYPOINTS).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da78237-2bc4-408b-8441-5ef3eb4106fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    for images, targets, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e2113-dd80-415e-9ab8-cdcebc7e6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fccdd4-66f1-41b0-9669-9d64f35c4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, dataset, idx):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img, _, keypoints = dataset[idx]\n",
    "        img_tensor = img.unsqueeze(0).to(device)\n",
    "        outputs = model(img_tensor)\n",
    "        output_heatmaps = outputs.squeeze(0).cpu()\n",
    "        \n",
    "        # Get predicted keypoints from heatmaps\n",
    "        pred_keypoints = []\n",
    "        for i in range(NUM_KEYPOINTS):\n",
    "            heatmap = output_heatmaps[i]\n",
    "            y, x = np.unravel_index(heatmap.argmax(), heatmap.shape)\n",
    "            confidence = heatmap.max()\n",
    "            pred_x = (x / OUTPUT_SIZE[1]) * INPUT_SIZE[0]\n",
    "            pred_y = (y / OUTPUT_SIZE[0]) * INPUT_SIZE[1]\n",
    "            pred_keypoints.append((pred_x, pred_y, confidence))\n",
    "        pred_keypoints = np.array(pred_keypoints)\n",
    "        \n",
    "        # Visualize Ground Truth\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_np)\n",
    "        x = keypoints[:, 0]\n",
    "        y = keypoints[:, 1]\n",
    "        v = keypoints[:, 2]\n",
    "\n",
    "        # Draw ground truth skeleton\n",
    "        for connection in skeleton:\n",
    "            p1, p2 = connection\n",
    "            if v[p1] > 0 and v[p2] > 0:\n",
    "                plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'r-', linewidth=2)\n",
    "        for i in range(len(x)):\n",
    "            if v[i] > 0:\n",
    "                plt.plot(x[i], y[i], 'bo', markersize=5)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Visualize Prediction\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(img_np)\n",
    "        x = pred_keypoints[:, 0]\n",
    "        y = pred_keypoints[:, 1]\n",
    "        c = pred_keypoints[:, 2]\n",
    "\n",
    "        # Draw predicted skeleton\n",
    "        for connection in skeleton:\n",
    "            p1, p2 = connection\n",
    "            if c[p1] > 0.1 and c[p2] > 0.1:  # Threshold confidence\n",
    "                plt.plot([x[p1], x[p2]], [y[p1], y[p2]], 'g-', linewidth=2)\n",
    "        for i in range(len(x)):\n",
    "            if c[i] > 0.1:\n",
    "                plt.plot(x[i], y[i], 'yo', markersize=5)\n",
    "        plt.title('Model Prediction')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3deb40a-8689-482a-bac6-f01ff3861e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO keypoint connections\n",
    "skeleton = [\n",
    "    [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "    [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "    [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "    [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "]\n",
    "\n",
    "# Visualize results on validation data\n",
    "for i in range(5):\n",
    "    visualize_results(model, val_dataset, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53101db2-ee39-40f7-9571-c378d64538d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'models/PoseNet_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e6ee3-f48d-4dc6-ba46-ab3ac42b267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_dataset = PersonKeypointsDataset(\n",
    "    annotation_file=VAL_ANNOTATION_PATH,\n",
    "    image_path=VAL_IMAGE_PATH,\n",
    "    min_keypoints=9,\n",
    "    max_samples=150,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('posenet_model.pth'))\n",
    "\n",
    "# Visualize results on test data\n",
    "for i in range(5):\n",
    "    visualize_results(model, test_dataset, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb4c6a-07ed-4265-8828-b1a162353d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
